# ğŸš€ Data Engineering Poftfolio
![Profile Views](https://komarev.com/ghpvc/?username=Mahesh2371&color=blue)

End-to-end Data Engineering platform demonstrating both **Real-Time Streaming** and **Batch Lakehouse ETL** architectures using modern Big Data tools.

This repository showcases production-style implementations using:

âœ… PySpark  
âœ… Kafka
âœ… SQL
âœ… Delta Lake  
âœ… AWS S3 (local simulation)  
âœ… Apache Airflow  
âœ… Docker  

---

## ğŸ“Œ Projects Included

### 1ï¸âƒ£ Streaming Pipeline
Kafka â†’ Spark Structured Streaming â†’ Delta Lake

â€¢ Real-time ingestion of transactions  
â€¢ Aggregations & transformations  
â€¢ Low-latency processing  
â€¢ Fault-tolerant checkpoints  
Folder: `/streaming-pipeline`

---

### 2ï¸âƒ£ Batch Lakehouse ETL
S3 â†’ Spark â†’ Delta â†’ Analytics tables

â€¢ Batch ingestion  
â€¢ Data cleansing & transformations  
â€¢ Star schema modeling  
â€¢ Airflow orchestration  
Folder: `/batch-lakehouse-etl`

---

## ğŸ—ï¸ Architecture

### Streaming
Producer â†’ Kafka â†’ Spark â†’ Delta Lake â†’ Analytics

### Batch
Raw â†’ Spark ETL â†’ Delta Lake â†’ Data Marts â†’ BI

---

## âš™ï¸ Tech Stack

| Category | Tools |
|----------|---------------------------|
| Processing | PySpark, Spark SQL |
| Streaming | Kafka |
| Storage | Delta Lake, S3 |
| Orchestration | Airflow |
| DevOps | Docker |
| Language | Python |

---

## ğŸš€ Quick Start (Docker)

### Step 1
```
docker-compose up --build
```

### Step 2
Run streaming producer
```
python streaming-pipeline/producer/transaction_producer.py
```

### Step 3
Run Spark job
```
spark-submit streaming-pipeline/spark/stream_processor.py
```

### Step 4
Access Airflow
```
http://localhost:8080
```

---

## ğŸ“Š Outcomes

âœ” Real-time processing  
âœ” Batch ETL  
âœ” Lakehouse architecture  
âœ” Production-ready design  
âœ” Resume-ready project

---

## ğŸ‘¤ Author
Mahesh S M  
Senior Data Engineer  
